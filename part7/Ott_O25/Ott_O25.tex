
\resetcounters

\bibliographystyle{asp2010}

\markboth{Ott}{Herschel Data Processing Development}

\title{Herschel Data Processing Development -- 10 years after}
\author{Stephan Ott}
\affil{Herschel Science Centre, European Space
Agency\altaffilmark{1}}
\altaffiltext{1}{Herschel is an ESA space observatory with science instruments provided by European-led Principal Investigator consortia and with important participation from NASA.}

\aindex{Ott, S.}

\begin{abstract}
The Herschel Space Observatory, the fourth cornerstone mission in the ESA science program, was launched on the 14th of May 2009. As a cryogenic mission Herschel's operational lifetime is consumable-limited by its supply of liquid helium, estimated to be depleted by March 2013. Afterwards the mission will start a 4.75~year long post-operations phase.

Originally it was considered sufficient to provide astronomers with raw data and software tools to carry out a basic data reduction, and no "data products" were to be generated and delivered. Following the realisation that the expectations of the astronomical community on the deliverables of an observatory mission evolved it was agreed to implement a single `cradle to grave' data analysis system supporting the needs of all users for the whole project cycle.

We will summarise the lessons learned during those ten years of Herschel data processing development, address the main challenges of this major software development project, reflect on what went well, what needed to be adapted and our open points.
\end{abstract}

\section{Introduction}
The Herschel Space Observatory, the fourth cornerstone mission in the ESA science program, was launched 14th of May 2009. With a 3.5~m telescope, it is the astronomical space telescope with the largest mirror ever launched. Herschel's three instruments (HIFI, PACS, and SPIRE) perform photometry and spectroscopy in the 55 - 671~micron range, previously unexplored ranges of the electromagnetic spectrum. Opening a new window to the universe Herschel delivers top-quality science for the astronomical community since the start of the routine phase in October 2009 \citep{Pilbratt10}.

In the original plans for the Herschel (at this time FIRST) science ground segment the FIRST Integrated Network and Data Archive System (FINDAS) was considered to be the heart of the whole ground segment \citep{bauer98}. FINDAS, later to be transformed into the Herschel Common Science System \citep[HCSS]{Riedinger09}, was expected to provide astronomers with raw data and a assortment of software tools to carry out data reduction. No scientific meaningful ‘data products’ were to be generated and delivered. Following the realisation that in order to increase the scientific return of an observatory mission a data analysis system and scientifically useful products must be provided to the astronomical community additional resources were made available to implement a freely distributable data processing system. The goal was to provide a single `cradle to grave' data analysis system which supports the needs of both the project team and the general astronomical community starting from early instrument level tests, covering pre-launch system operational verification tests, check-out and performance verification phase, operations, post operations and finishing with the population of the Herschel legacy archive.

The Herschel Data Processing system is a major project with over 200~contributors distributed all over the globe. With 3.4~million lines of code (MLoC) (2.5~MLoC in the main branch, and 0.9~MLoC for unit tests) the code comprises 75~\% of the complete HCSS. The remaining 25\% is mainly code to support the Versant database and uplink.

\section{The original plans -- what went well}
The Herschel Science Centre, the HIFI, PACS and SPIRE Instrument Control Centres (ICCs) and NHSC jointly manage and contribute to the Herschel Data Processing System \citep{ott10}. Delivery based contracts were awarded to the Instrument Control Centres to provide software to generate scientific useful data products and data visualisation and reduction tools.

The Herschel data processing development followed HCSS' concept of smooth transition between the various project phases and an iterative and incremental (agile) development cycle. The data processing development management team was supported by a system architecture group (for technical advice), a users' group (for user feedback) and several Configuration Control Boards (CCBs) steering the day-to-day development.

The decision to combine data retrieval, pipeline execution and interactive tools (both general viewing and analysis tools and expert applications like instrument calibration) in one single environment was correct. Algorithms developed and validated in the interactive environment could be readily applied to the processing pipelines. The same holds for providing a software system that does not depend on commercial licenses, and giving the community access to the latest development builds and the source code, putting the general community on par with the project experts (and vice versa!).

Three years before launch we conducted our first end-to-end test, set up to verify that the data processing system is capable of processing raw telemetry into products, and ingest them into the Herschel Science Archive. While at this first campaign pipelines had to be simulated by interactive processing it set the scene for the following seven, increasingly realistic, campaigns which ensured we were ready for operations. Five months before launch we had our first data processing workshop paving the way to the Herschel First Science Highlights conference held only seven months after launch where an unprecedented number of more than 200~science papers was presented.


\section{During the 10~years of development and operations -- what was adapted}

After the data processing development started in earnest it became obvious that the existing HCSS infrastructure could not meet the demands of a major distributed software development project. The home grown software problem reporting system did not provide sufficient functionality and needed to be replaced with a professional issue and project tracking software. The original build system compiled all deliveries together once a day. This led to serious problems to integrate all deliveries that had piled up and now failed to compile. Major efforts were needed to disentangle all incompatibility issues to get a successful build. Therefore a Continuous Integration Build (CIB) was put into place in which a build and unit test cycle is triggered each time a new software delivery is made, and failed deliveries are quarantined. The CIB was also instrumental to re-unite the various HCSS flavours developed by the three ICCs back into one common system, and to provide the base for a user installer. To increase the depths of testing we added an integration tester that runs parts of the system test, whole pipelines and user scripts three times a day.

As time progressed we embraced new technologies like TWiki, WebEx, support to plug-ins and integrated access to the Virtual Observatory and to data in the Herschel Science Archive. Most of the time we remembered that we need to be task, not interest driven, to avoid spending a lot of resources in minor but interesting looking areas.

On the organizational side it was realized that we lacked a dedicated Data Processing scientist with the role to channel users' experiences and requirements into constructive feedback to steer further development. We realized the need to increase the contact with our customers by offering direct feedback channels on code and documentation, and transforming the ``developer only'' Common Science Development Team meetings to HIPE Fora where user representatives are explicitly encouraged to attend. Equally essential was the employment of a professional technical author. Beyond significantly increasing the quality and consistency of the documentation a technical author is aware of the state-of-the art of documentation which led us to the use of video tutorials, social media and instant feedback. The employment of a schedule engineer to help management with spreadsheets, Gantt charts and to prepare progress reports was helpful, too.

As management also needs to ensure compliance with non-functional requirements like maintainability, stability, adaptability and responsiveness software quality assurance plays a major role in any big software project, with a competent quality assurance engineer recommending process improvements. For the HCSS this was e.g.\ the introduction of a patching procedure for the operational track,  the introduction of the system CCB charged to monitor the transition from a development version to a tested system and the core CCB which is in charge of the overall configuration control of the science ground segment.

\section{Reflecting 10~years of development -- what should have been adapted}

\begin{itemize}
\item document all key decisions to reduce rewriting of history in a later stage
\item have regular, bi-weekly progress teleconferences to report progress and give a forum to discuss and resolve current issues
\item have regular code reviews, specially for packages where a lot of development is expected and where the automatic code quality checker indicates issues
\item introduce an automatic code quality check as part of the build system that determines whether new code meets the code quality standards, and can be uploaded and integrated. Introduce this well before the ''craze for functionality'' period
\item ensure the role of users' group is clearly defined as ''product owners'' to provide concrete and specific user requirements to drive the development. Involve and empower non-project community as soon and much as possible to avoid that in-project interests prevent addressing of issues that are highly relevant for the astronomical community
\item give the community direct visibility of, and the possibility to open software tickets addressing both problem reports and change requests
\end {itemize}

\section{Reflecting 10~years of development --  remaining open points}
\begin{description}
\item {\bf Development environment:} Should we have a mandatory development environment for all developers? What level of training should be requested?
\item {\bf Distribution of resources:} Given a fixed level of resources how to balance best the resources for testing with the resources for development?
\item {\bf Evolution:} How to handle deprecations, replacements and standardization for a system that evolves over a decade as much needed improvements will break existing functionality?
\item {\bf Planning:} What is the appropriate level of detail for requirements documentation and work package descriptions? How much forward planning -- both in duration as in detail -- can be performed and maintained without spending major efforts into the pure maintenance of plans?
\item {\bf Frequency of releases:} What is the right compromise to balance validation efforts vs.\ users' expectation to have the latest and greatest functionality available in a tested release? How to balance the weariness of the community getting too frequent releases vs.\ the productivity boost close to a deadline?
\item {\bf Involvement and recognition (gamification):} How to involve the users' community for voluntary contributions and testing? Also within the project how can we reward our most active and willing contributors?
\item {\bf Decision authority:} What is the right level where fiercely debated technical decisions are decided to avoid that those disagreements percolate up to the highest project management level?
\item {\bf Consistency in overall development:} How can we ensure to have a common look and feel for GUIs and command line tasks over the whole system without stifling, outdated, and not-adhered-to coding guidelines? How to ensure that good ideas are made public, and discussed before being implemented setting a de-facto standard? How to avoid parallel developments taking place in different locations?
\end {description}

\acknowledgements The author would like to thank all dedicated contributors to the Herschel Data Processing System who worked on calibration, coding, documentation, management, management support, pipeline operations, quality assurance, quality control, testing and tutoring (http://herschel.esac.esa.int/Acknowledgements\_files/sheet004.htm). The success of the Herschel Data Processing System is your success.

\bibliography{editor}
