% This is the aspauthor.tex LaTeX file
% Copyright 2010, Astronomical Society of the Pacific Conference Series

\documentclass[11pt,twoside]{article}
\usepackage{asp2010}
\usepackage{graphicx}
\usepackage{floatrow}
\resetcounters

\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1ex}{0ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}

\markboth{Young, M.D., Gopu, A.,  Hayashi, S, and Cox, J.A.}{}

\begin{document}

\title{%%TODO: Better name
FRIAA: A FRamework for web-based Interactive Astronomy Analysis using AMQP messaging}
\author{Michael D. Young$^1$, Arvind Gopu$^1$, Soichi Hayashi$^1$, Jeffrey A. Cox$^2$}
\affil{Indiana University, [2709$^1$, 2719$^2$] E. 10th St, Bloomington IN 47408}

\begin{abstract}

This paper describes a web-based FRamework for Interactive Astronomy Analysis (FRIAA) being developed as part of the One Degree Imager - Pipeline, Portal, and Archive (ODI-PPA) Science Gateway. The framework provides astronomers the ability to invoke data processing modules including IRAF and SExtractor on large data within their ODI-PPA web account without requiring them to download the data or to access remote compute resources. Currently available functionality includes contour plots, point source detection and photometry, surface photometry, and catalog source matching. The web browser front-end developed using the Zend PHP platform and the Bootstrap library makes Remote Procedure Calls (RPC) to the back-end modules using AMQP based messaging. The compute-intensive data processing codes are executed on powerful and dedicated nodes on a compute cluster at Indiana University. 

\end{abstract}

\section{Introduction \& Justification}
\label{section:intro}

The WIYN Consortium, Inc.\footnote{The WIYN Observatory is a joint facility of The University of Wisconsin-Madison, Indiana University, Yale University, and the National Optical Astronomy Observatory (NOAO).} 3.5m telescope's One Degree Imager (ODI), with a fully populated focal plane, will produce raw observational data on the order of 2-4 GB per exposure, and approximately 0.5 TB for a typical 3 night observational run. Standard pipeline processed calibrated data is also of the same order of magnitude in size. The traditional astronomy data processing paradigm is predicated on the user downloading the raw exposure dataset using FTP or from portable media, and then performing initial calibration as well as advanced analysis on powerful desktop computers or other personal computing infrastructure. While personal computers have steadily become more powerful and capable, the aforementioned paradigm still was identified by WIYN as not scalable to ODI's data sizes or computational needs. 

The ODI Pipeline, Portal, and Archive (ODI-PPA) web-based Science Gateway \footnote{The ODI-PPA system is a joint development effort between Indiana University's Research Technologies division and Pervasive Technology Institute (PTI) in partnership with WIYN, and the National Optical Astronomy Observatory.} (\url:http://portal.odi.iu.edu) is being developed to serve as the primary user interface for astronomers using ODI, and enable a paradigm shift in astronomy data access and processing methodology.  It consists of a web browser front-end developed using the Zend PHP platform and the Bootstrap library paired with a MySQL backend.    

\section{Design and Structure of %TODO:NAME 
FRIAA}
\subsection{Advanced Message Queuing Protocol (AMQP) based Messaging}
\label{section:amqp}

The heart of FRIAA is an Advanced Message Queuing Protocol (AMQP) server (\cite{amqp}) paired with a shared networked scratch space (see Section \ref{section:dss}).  We have chosen to use the RabbitMQ (\url{http://www.rabbitmq.com}) open-source implementation of AMQP.  Various components of ODI-PPA publish messages to pre-defined queues on the AMQP server, and listen for acknowledgments or follow-up messages from other components.    Figure \ref{fig:workflow} illustrates the central role played by the AMQP server in enabling the flow of data movement and processing from one component to the next.  

The asynchronous and First-In First-out (FIFO) nature of AMQP-based messaging simplifies addition or removal of resources to various points within the framework.  Queues can be monitored for message backlogs, and idle resources can be brought to bear by spawning additional listener processes, to pull the next available job from the message queue.  Any component capable of sending/receiving messages through the AMQP server, and has access to shared scratch space, can be added seamlessly to the framework, allowing for the efficient re-tasking of limited compute resources.

\subsection{Portal and Image Explorer}
\label{section:portal}

Image Explorer is a sub-component of the Portal dedicated to displaying raw and reduced astronomical images to the user and enabling access to various analysis tools (described further in an upcoming paper).   Built using HTML5 technology and Javascript, Image Explorer allows users to interact with their data from most modern web browsers without installing any additional software or the need to download the FITS image data.

\subsection{Data Subsystem}
\label{section:dss}

The ODI-PPA Data Subsystem (DSS) is a distributed set of asynchronous components that coordinate archival and retrieval of data products via an Esper (\url{http://esper.codehaus.org}) event stream. The DSS archives data products to a High Performance Storage System (HPSS) based tape archive (\cite{hpss}) at Indiana University and makes direct access to the archived data products available via shared ODI-PPA working space on the Indiana University Data Capacitor (DC) (\cite{dataCapacitor}).  Movement of data products between HPSS and DC is managed using the iRODS data mover service (\url{http://irods.sdsc.edu}).  

\subsection{Dedicated Computing Hardware}
\label{section:wn}

A limited set of compute nodes on the Quarry Supercomputing Cluster at Indiana University have been reserved for use as worker nodes for %TODO:NAME.
FRIAA.  These nodes have a 6-core Intel Xeon 2.40GHz CPU (E5645), 48GB of memory, and run Red Hat Enterprise Linux (RHEL) 5.8.  Optimized versions of astronomical packages, including SExtractor (\cite{sextractor}), and IRAF (\cite{iraf}) have been installed on these nodes. 

\section{Workflow Sequence}
\label{section:workflow}

\begin{figure}
\includegraphics[width=0.5\textwidth]{O15_f1.eps}
\caption{Schematic of user-initiated analysis workflow within the ODI-PPA system.  Dashed lines indicate messages passed through the AMQP Server. See Section \ref{section:workflow} for more information.}
\label{fig:workflow}
\end{figure}

Numerical identifiers in this section refer to markers shown in Fig \ref{fig:workflow}.  When the user requests a process to be run upon their selection of data (1), the Portal registers this request, issues a data thaw (staging) request through the AMQP server (2).  DSS, listening to the thaw.request AMQP queue (3), stages the requested data off the HPSS tape archive (4) to DC (5), if necessary, and then acknowledges the original thaw.request message (6).  Upon receiving this (7), the Portal prepares and places instructions to run a worker node job on DC (8).  It then sends a message to the worker.runjob queue (9).   The first available worker node process retrieves the message including the job id (10), and the associated execution instructions (11). Then it executes the job, and places the results in an appropriate shared directory on DC (12).  Finally, the worker node sends an acknowledgment to the AMQP server in response to the original worker.runjob message (13). The Portal listens to this message (14), and notifies the user about completion of their job along with access to the results (15).  

\subsection{Workflow Example: SExtractor}
\label{section:sextractor}

\begin{figure}[ht]
\includegraphics[width=0.5\textwidth]{O15_f2.eps}
\caption{Interface to the SExtractor tool within the Portal's Image Explorer (Section \ref{section:portal}), showing a sampling of user-configurable parameters.}
\label{fig:sextractor}
\end{figure}

The SExtractor tool is designed to identify and quantify sources within astronomical images.  The SExtractor interface within the Portal's Image Explorer (Section \ref{section:portal}) is shown in Figure \ref{fig:sextractor}.  The user can alter various parameters that are usually set in a .config file specified at runtime.  Runtimes for SExtractor can vary greatly depending on the size, depth, and nature of an image.  

Here we compare the processing times from identical runs of SExtractor: one running through the workflow as described in Section \ref{section:workflow} within ODI-PPA; The other running on a typical desktop workstation (Intel Core 2 Duo 3.0GHz E6850 CPU, 8GB RAM). The example image we are using for the following test is a crowded-field image of M15 observed on ODI, totaling 3.05GB in size. We used the latest stable version (2.8.6) of SExtractor and default parameters, on both platforms.

\begin{figure}[ht]
\includegraphics[width=0.75\textwidth]{O15_f3.eps}
\caption{SExtractor processing time on ODI-PPA vs. Desktop Workstation}
\label{fig:sex_run}
\end{figure}

SExtractor identified 56,703 sources within the image. It took 64 seconds on the worker node versus 73 seconds on the desktop workstation.  While the computational time is only marginally lower on our worker node, the inclusion of data download times in our analysis highlights the clear advantage of our design.  The FRIAA overhead involved with this operation through the Portal amounted to 4 seconds, versus 987 seconds on the desktop to download the 3.05GB .fits file (download speed: ~25Mbps.)  The results of this test are plotted in Figure \ref{fig:sex_run}.

\section{Conclusion and Future Work}

The FRIAA framework offers astronomy data analysis capabilities on large data based on a modern web-standard interface, a reliable archive system and shared networked scratch space, and a group of compute nodes configured for astronomical analysis and accessible via a robust and reliable AMQP-based messaging setup. 

Our future plans include enhanced scalability via automation of spawning of additional processes to handle high demand events.  We also plan to expand the suite of astronomy tools, including all of the standard IRAF analysis packages.  We plan to display results interactively through the Portal Image Explorer, where feasible; for example, the ability to mark SExtractor results on its input image and allowing the user to add or remove sources manually.  Our framework already provides the bulk of the groundwork necessary to make these enhancements.  

 
\acknowledgements 
This material is based upon work supported by the National Science Foundation under Grant No. CNS-0723054.  
%%need this for quarry

\bibliographystyle{asp2010}
\bibliography{../../editor}  

\end{document}
