
\resetcounters

\bibliographystyle{asp2010}

\markboth{Dowell}{LWA1 Software}

\title{Software and Computing at LWA1}
\author{
Jayce~Dowell$^1$ on behalf of the LWA Collaboration$^2$
\affil{$^1$University of New Mexico, Department of Physics and Astronomy, 1919 Lomas Blvd NE,  Albuquerque, NM 87131}
\affil{$^2$\url{http://lwa.unm.edu/}}
}

\aindex{Dowell, J.}

\begin{abstract}
The first station of the Long Wavelength Array, LWA1, is a new low frequency radio telescope currently operating in New Mexico, USA.  The primary data products delivered by the telescope consist of raw voltage time series data.  As such, the data have large volumes with a high degree of flexibility.  The LWA Software Library (LSL) is designed to process these data and provides a variety of signal processing functions that are not generally available.  LSL also serves as the software available on the LWA1 User's Computing Facility, a six node cluster available to LWA1 observers for data reduction.
\end{abstract}

\section{Introduction}
The first completed station of the Long Wavelength Array \citep[LWA;][]{LWA}, LWA1, is currently operating in New Mexico in the 10 to 88 MHz frequency range.  LWA1 is co-located with the Jansky Very Large Array (VLA) and consists of 257 crossed polarization dipole antenna pairs located within an approximately 100 by 110 meter ellipse with an ``outrigger" antenna pair located approximately 300 meters to the east.  Signals from these antenna pairs are combined within the digital processing electronics to provide raw time series voltages from all dipoles as well as from four independently steerable delay-and-sum beams.  These two modes support a variety of scientific research, including studies of the decametric emission from Jupiter at high temporal and spectral resolution, searches for the first stars through their influence on the HI spin during the cosmological ``dark ages", and searches for radio transients \citep[see][and references therein]{FL}.

\section{The LWA Software Library}
\subsection{Overview}
In order to take advantage of the flexibility of the voltage data, and to handle the complexities of large-N and large data volumes from the telescope, we have developed the Long Wavelength Array Software Library \citep[LSL;][]{LSL}.  Unlike other software packages for reducing astronomical data, e.g., CASA\ooindex{CASA, ascl:1107.013}  \citep{CASA}, LSL is not an all-inclusive environment.  Instead LSL provides user's with data reduction ``building blocks".  LSL is written as a Python module with C extensions that enable the flexibility of an interpreted and dynamically typed language to be combined with the speed of a compiled language.  The software is distributed under the terms of version two of the GNU Public License and is available for Linux, OS X, and FreeBSD platforms at \url{http://fornax.phys.unm.edu/lwa/trac/wiki}.

LSL consists of four key components:  data readers and writers, metadata extraction routines, basic analysis routines, and general-use functions.  The data readers and writers provide methods for extracting the binary-packed LWA1 data into NumPy \citep{Numpy} arrays while the writers allow for data processed within Python to be exported to other environments.  The metadata utilities help combine the various sources of metadata with the data in order to match observations to particular sources.  The metadata functions also handle the creation and parsing of the observation schedules.

A variety of functions are included within the analysis routines to allow observers to convert the data into the frequency domain and to perform a variety of signal processing techniques, including post-acquisition beam forming from the ``all-dipoles" data and removal of broadband RFI in the time domain data.  The ability to work with time series voltage data is one of the key distinctions between LSL and other software libraries.  Finally, LSL includes a variety of general purpose routines that have a wide range of applications.  These extend from functions for simulating the low frequency sky based on the Global Sky Model \citep{GSM} to mathematical routines for interpolation and smoothing. 

Figures \ref{P42_fig:sky} and \ref{P42_fig:psr} demonstrate the flexibility of LSL through two different data processing approaches.  Figure \ref{P42_fig:sky} demonstrates interferometry and shows an all-sky image created using the \\{\tt lsl.correlator.fx.FXMaster()} software correlator function, using data from the transient buffer mode.  A single dish-style beam former detection of the pulsar B1919+21 is shown in Figure \ref{P42_fig:psr}.  In this case, the {\tt lsl.misc.dedispersion.coherent()} function is used to apply coherent dedispersion to the time series data before channelization and folding.

\begin{figure}
	\plotone{part10/Dowell_P42/P42_f1}
	\caption{\label{P42_fig:sky}37--38 MHz Stokes I image of the sky visible to LWA1 created using the LSL software correlator and data from the LWA1 transient buffer.  The image is a composite of 96 snapshots of 61 ms each and clearly shows the Galactic plane as well as Cas A and Cyg A.}
\end{figure}

\begin{figure}
	\plotone{part10/Dowell_P42/P42_f2}
	\caption{\label{P42_fig:psr}Detection of pulsar B1919+21 from $\sim$800 seconds of data at 60 MHz taken with a bandwidth of approximately 4 MHz.  The data where dedispersed using the LSL coherent dedispersion routine and then folded at the pulsar's period.}
\end{figure}

\subsection{Extensions}
In addition to the core functionality provided with LSL, there are several extensions that provide more specific functionality.  These extensions are developed along side LSL and  are updated to maintain compatibility with the latest version.  By keeping the extensions separate from the main body of LSL, the extensions can be updated at a faster pace in response to user needs.  Of the four extensions currently available, two of particular interest are the {\tt Commissioning} and {\tt GPU} extensions.

The {\tt Commissioning} extension is a collection of utilities used by LWA1 for data analysis during commissioning and contains scripts that are most similar to what can be found in standard reduction software such as CASA\ooindex{CASA, ascl:1107.013} .  For example, the {\tt hdfWaterfall.py} script takes a data file created by one of the beam formers, unpacks and organizes the data, transforms it to the frequency domain, and writes the integrated spectra to an HDF5 file.  

 The GPU extension is designed to take advantage of new computing capabilities available to LWA1 observers through the LWA1 User's Computing Facility (see \S\ref{P42_sec:LUCF}).  This extension uses the framework provided by the PyCUDA \citep{PyCUDA} Python module to move the signal processing from the CPU to the highly parallel GPU.  Thus, more complex signal processing can be applied to the data with no overall increase in task runtime.

\section{\label{P42_sec:LUCF}The LWA1 User's Computing Facility}
Although the LWA1 is co-located with the VLA and connected to the outside world via a 1GbE link, the potential data rate of $\sim$1 TB per hour for beam formed data creates logistical problems for moving and processing data.  The current approach employed by LWA1 is to perform weekly or biweekly swaps of the data recorder storage units out at the site.  Once the data arrive back at the University of New Mexico, the data are copied off to external eSATA drives and distributed to observers.  This procedure incurs a sizable logistics overhead and increases data delivery times.

The LWA1 User's Computing Facility is designed to help alleviate the data transfer problem by providing computing resources located close to where the data are recorded.  The computing facility is a cluster of  six machines designed for signal processing applications.  Each node contains a 3.2 GHz hexacore processor, 32 GB of RAM, 12 TB of internal storage, and two GPUs.  The nodes are connected together using a 10 GbE {\it ad hoc} spanning tree network and will be connected to the LWA1 site via a 10 Gb/s link.  The novel network approach used for the cluster helps reduce costs by eliminating the need for an expensive 10 GbE switch.  Once connected, this will enable a variety of real-time and off-line processing options for the LWA1 observer as well as serve as a test bed for correlating data from future LWA stations with LWA1.

\acknowledgments Construction of the LWA has been supported by the Office of Naval Research under Contract N00014-07-C-0147.  Support for operations and continuing development of the LWA1 is provided by the National Science Foundation under grant AST-1139974 of the University Radio Observatory program.

\bibliography{editor}
