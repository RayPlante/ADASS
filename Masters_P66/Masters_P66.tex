% This is the aspauthor.tex LaTeX file
% Copyright 2010, Astronomical Society of the Pacific Conference Series

\documentclass[11pt,twoside]{article}
\usepackage{asp2010}

\resetcounters

\bibliographystyle{asp2010}

\markboth{Joe Masters and Jim Braatz}{A Spectral Pipeline for the Green Bank Telescope}

\begin{document}

\title{A Spectral Pipeline for the Green Bank Telescope}
\author{Joe~Masters$^1$ and Jim~Braatz$^1$
\affil{$^1$NRAO Headquarters, 520 Edgemont Rd, Charlottesville, VA 22903-2475}}

\begin{abstract}
Recently, history Green Bank Telescope (GBT) data was made available to the user community through the National Radio Astronomy Observatory (NRAO) data archive service.  In support of this archive, and to offer quick access to GBT data, NRAO has instituted the GBT Pipeline Project to provide reduced spectra from public GBT datasets to the user community.
\end{abstract}

\section{Introduction}

The GBT Pipeline Project has two goals. The first is to produce an automated data processing pipeline that can generate quick-look spectra for about 80\% of all spectral line data observed with the GBT, including data observed with the VEGAS spectrometer \citep{2012arXiv1202.0938A} in standard observing modes. For certain types of science data, this project will produce archive-ready data products, including a ``summary sheet" for each reduced spectrum with accompanying header and statistical information. A second goal of the Pipeline Project is to prepare tools that assist with certain potentially labor-intensive data processing tasks, such as calibrating and mapping K-band Focal Plane Array observations \citep[see][]{2011ASPC..442..127M}.

Through various approved observing programs, the GBT has taken pointed observations to measure HI profiles toward several thousand galaxies. Here we show early results of the quick-look pipeline for extragalactic HI observations. The GBT Pipeline project plans to provide HI spectra of galaxies to the NRAO data archive and to the NASA Extragalactic Database this year.

\section{RFI Excision}

Most extragalactic HI spectra are affected by RFI at some level.  The ``before and after" plots in Figure~\ref{fig:RFIfigure} demonstrate the effectiveness of our initial RFI flagging approach for particularly heavy RFI.  We have been developing the quick-look pipeline for only a few weeks, so the automated RFI excision we apply is rudimentary, but already effective. We examine each integration for broadband RFI (excessive spectral baseline ripples) and flag integrations based on the total power in low-order Fourier components. We then calibrate and average the data. Finally we flag narrow-band RFI using an iterative, median filter approach.

\begin{figure}[ht!]
\centering%
\includegraphics[width=5in]{P66_f1}
\caption{RFI excision of three exmample spectra.  Plots on the left are before RFI exicsion and plots on the right are the removal of bad component spectra and/or bad channels.}
\label{fig:RFIfigure}
\end{figure}

\subsection{Baseline Subtraction}

Like RFI exicsion, the best methods for baseline subtraction will take some experimentation.  However, baseline subtraction in the pipeline will likely consist of:

\begin{itemize}
\item a selectable baseline fitting range (i.e. channels, velocity)
\item a tunable order of polynomial for fit
\item a choice of sinusoidal or fourier fitting
\end{itemize}

\subsection{Data Grouping and Selection}

The pipeline will allow users to select which data to process.  In some cases, this will be a filtered set of data.  In other cases, it will simply be a way of grouping data together during processing without any reduction in the total amount of data being processed.  For example, the user may process every scan pair or all scans for a target in an observation.  Furthermore, a user may choose either to separate or combine polarizations in their processing.  Other processing filters may include the source name, the spectral window or included/excluded regions of spectra (e.g. by channels or velocity).

\section{Automatic vs Manual Operation}

\paragraph{Automatic}

The first, and in some ways most sophisticated mode, is for the pipeline to run without user input.  Ideally, the pipeline will run automatically at the completion of an observation.  Alternatively, the user should just direct the pipeline to a particular input file or even a Project Identifier.  In this mode, the pipeline will use information contained within the dataset(s) to make educated guesses about what calibration techiniques and parameter values are appropriate for the observation.

A user may use the automatic mode on an entire dataset or a subsection of integrations.

\paragraph{Manual}

The user will also be able to run the pipeline with full, or close to full, control over many calibration and output settings.  This may be useful when the user believes she can make better choices about parameter values.

\section{Data Products}

By default, the pipeline will produce the following data products.  Depending on options set by the user, the format of each of these products may vary.
\begin{enumerate}
\item Calibrated single dish FITS (SDFITS) with a spectrum per target per spectral window
%        + multi-beam scenarios are a special case to be handled differently
\item A summary web page including images of spectra, header information and basic statistics/analysis
\end{enumerate}

These products are also oriented toward use by the archive.  They will be searchable by header information in the SDFITS file and in the summary page.

\subsection{Summary Information}

The pipeline will produce a summary page (or set of summary pages) for each processing run.  The format may be PDF, HTML or plain text.

It will include:
\begin{itemize}
\item a plot of each calibrated spectrum with high-level metadata for the dataset according to the data selection parameters.  By default, this will produce a spectrum for each target.
\item Weather information
\item Basic statistics
\end{itemize}

\section{Future work}

The intitial approach is to run the pipeline on the Green Bank network.  At a later stage, the pipeline may be controlable from a web interface or able to be run on a user's home computer.

Likewise for data product retrieval.  Initally the data produced by the pipeline will be accessable on the Green Bank network.  Later, data procucts may be retrieved either from an archive access interface or another web interface.  If the pipeline is run on a user's home computer, then the data products will be produced on that machine.

\acknowledgements We would like to thank the National Science Foundation for funding this work.

\bibliography{P66}

\end{document}
