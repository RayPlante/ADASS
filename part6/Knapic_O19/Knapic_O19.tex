
\resetcounters


\markboth{Knapic, Molinaro, and Smareglia}{Full Tolerant Archiving System}

\title{Full Tolerant Archiving System}
\author{Cristina~Knapic, Marco~Molinaro, and Riccardo~Smareglia
\affil{INAF Astronomical Observatory of Trieste, Italy}}

\aindex{Knapic, C.}
\aindex{Molinaro, M.}
\aindex{Smareglia, R.}

\begin{abstract}
The archiving system at the Italian center for Astronomical Archives (\ssindex{archives!multiple!Italian Astronomical Archive (IA2)}IA2) manages data from external sources like telescopes, observatories, or surveys and handles them in order to guarantee \ssindex{data!preservation}preservation, dissemination, and reliability, in most cases in a \ssindex{Virtual Observatory (VO)}Virtual Observatory (VO) compliant manner. A \ssindex{data!metadata}metadata model dynamic constructor and a data archive manager are new concepts aimed at automatizing the management of different astronomical data sources in a fault tolerant environment. The goal is a full tolerant archiving system, nevertheless complicated by the presence of various and time changing data models, file formats (\ssindex{data formats!FITS}FITS, \ssindex{data formats!HDF}HDF5, \ssindex{data formats!ROOT}ROOT, \ssindex{data formats!PDS}PDS, etc.)\footnote{Flexible Image Transport System (\ssindex{data formats!FITS}FITS), Hierarchical Data Format (\ssindex{data formats!HDF}HDF), Planetary Data System (\ssindex{data formats!PDS}PDS)} and \ssindex{data!metadata}metadata content, even inside the same project. To avoid this unpleasant scenario a novel approach is proposed in order to guarantee data ingestion, backward compatibility, and information \ssindex{data!preservation}preservation.
\end{abstract}

\section{Introduction: Issues and Constraints}
The Italian center for Astronomical Archives (\ssindex{archives!multiple!Italian Astronomical Archive (IA2)}IA2) infrastructure manages raw data and storage facilities principally for \ssindex{observatories!Earth-based!LBT}LBT, \ssindex{observatories!Earth-based!TNG}TNG and \ssindex{observatories!Earth-based!Asiago Astronomical Observatory}Asiago Observatories; the request to archive and store calibrated data for European Surveys is a recent request. The evolution of data formats and the management of calibrated data emphasizes the need for new tools and lead to a revision of the software architecture towards more flexible solutions.

\subsection{Issues}
The main issues/desiderata may be summarized as follows:
\begin{itemize}
\item \ssindex{data!metadata}metadata change in content, format type, and header completeness whereas they should be stable
\item data and \ssindex{data!metadata}metadata may use different formats than \ssindex{data formats!FITS}FITS; more in detail, other formats (ASCII or PH) may be used for calibrated data in addition of \ssindex{data formats!FITS}FITS files, astronomical data can be stored in \ssindex{data formats!HDF}HDF5, \ssindex{data formats!PDS}PDS, \ssindex{data formats!ROOT}ROOT and other formats last but not least, the \ssindex{data formats!FITS}FITS standard evolves
\item difficult software reutilization between different purpose surveys
\item data type, although may affect the choice of database, should not affect the software or the system architecture
\item effective scalability is a key feature
\item the data \ssindex{data!pipelines!reduction}pipeline should access the database and the storage in a transparent way, independent of their location
\item the data policy and the version of the \ssindex{data!pipelines!reduction}pipeline should be defined in a reversible way
\item the archives may be geographically distributed in several sites (e.g. LBT)
\end{itemize}

\section{Constraints}
The proposed architecture should implement and natively support some new desirable features:
\begin{itemize}
  \item distributed systems
  \item different programming languages
  \item objects deployment shouldn't affect the archiving system performances
\end{itemize}
The Object Request Broker (ORB) architecture, that allows interoperation between distributed and multilanguage objects, and the Component Container paradigm allow the desired flexibility.

\subsection{Use Cases}
Some application use cases are presented and the features related to the Archiving system discussed.
\begin{itemize}
  \item raw data distribution; the critical aspects are data flushing towards several geographically distributed sites and data consistency
  \item calibrated and reduced data; all relations between raw, intermediate, and final products are stored in the header in both hierarchical and mixed ways; consistency is guaranteed by \ssindex{data!pipelines!reduction}pipeline developers but not guaranteed between several \ssindex{data!pipelines!reduction}pipeline versions
  \item real-time processing of large amounts of data; thus, more than one archiving system may be needed to keep up with the different data types: raw data, several level intermediate products, science-ready data products, and\ssindex{Virtual Observatory(VO)} VO publishing data products
\end{itemize}
In addition, archive managers need to be aware of all external information available in the project proposals, i.e. policy of publication, private data management, access permissions rules, etc. 

\section{Templates, Models, and Modules}
\ssindex{archives!multiple!Italian Astronomical Archive (IA2)}IA2 is intended to be a gateway between observing facilities and user science, allowing the latter to easily extract the needed information at every level. Centralizing the information on how the different topics should work, storing in a database, and using it to make an archive system container are essential steps to manage and control the data flux.

\subsection{Modules}
Four main modules have a big role in this schema: data modeling, \ssindex{data!metadata}metadata ingestion, data storage, or \ssindex{data!metadata}metadata and data retrieval. The Archiving system could be considered to be a container of different components that interact with each other. Some components may also be containers of smaller components. Each container is in charge of coordinating the relations and references between components, i.e. message exchange, component behaviour, and actions on files and storage. The top level container is the Archiving System Coordinator (ASC) which will contain such components as the File Identifier and Notifier, Data Modelization (DbCreator and DataFlux), and Project Manager (PM). The PM interacts directly with data and data flux models in order to sort the information and forward it to the proper destination. Other components will be the  Metadata Inserter, File Archiver,\ssindex{Virtual Observatory(VO)} VO Publisher, Policy Manager, \ssindex{data!pipelines!reduction}Pipeline User and Updater. The ASC is in charge of coordinating the actions depending on data and data flux models, checking allowed or denied actions as well. Data and data flux models should be created dynamically from data and, possibly, from small configuration files (or from a \ssindex{data!pipelines!reduction}pipeline analyzer tool) and stored in a dedicated database or an \ssindex{data formats!XML}XML schema. The advantages and disadvantages of the two methods are summarized in Figure~1.

\articlefigure[]{part6/Knapic_O19/db_xml.eps}{fig:dbvsxml}{Advanteges and disadvantages of using a database or \ssindex{data formats!XML}XML schema to store data and data flux model information.}

The database solution seems to be more flexible to keep up with model content evolution and easily tunable; also allows to avoid the additional conversion of the \ssindex{data formats!XML}XML schema into the database schema.

\subsection{Data and  Data Flux Models}
The centralization of information is a key point for the correct configuration of all the actors involved in the Archiving System. Job parameters have to be abstracted from code development, and stored in a dedicated database using two tables for Data Model and Data Flux Model information respectively. The first describes the type and format of the data that have to be ingested in the Archive, which libraries to use, technical information on \ssindex{data!metadata}metadata types, hierarchical structures, references to other files, etc. The second describes how relations between files work, where and when data should be stored and flushed, how it should be ingested, which \ssindex{data!pipelines!reduction}pipeline version has been used, and how to store \ssindex{data!metadata}metadata in the database. Also, the association between DBMS and data, the policy on data dissemination, ownerships, aliases and so on belong to the second table.

\articlefigure[]{part6/Knapic_O19/framework.eps}{fig:framework}{Framework Interfaces representation and interactions.}

\ssindex{data!metadata}Metadata archiving may involve different DBMS's and rely on different geographical sites. The DbCreator is a tool under development that maps \ssindex{data!metadata}metadata keys from new files reading all \ssindex{data!metadata}metadata available in the data file header and assigning the correct type, structure, and descriptors (UCDS for the\ssindex{Virtual Observatory(VO)} VO). These information are stored as part of the Data Model schema. A \ssindex{software!user interfaces}graphical user interface (DataFlux) helps the administrators to correctly configure the data flux schema in agreement with a pipeline analyzer tool. The Project Manager that manages all the information stored in the database and coordinates all the subsystems that are in charge of inserting \ssindex{data!metadata}metadata in the DB, store, manipulate, publish, or flush data over the system in a transparent way.

\subsection{Conclusion}
A \ssindex{computing!CORBA}CORBA based framework is a useful tool to hide the complexity of the whole Archiving system; several frameworks are mature and well suited to this development . The proposed approach is a complete redesign of the Archiving system, which is challenging and time consuming, but can dramatically improve the confidence in the whole archiving system.

\acknowledgements The authors would like to thank L. Pivetta for useful conversations and hints, A. Micol, F. Stoehr , P. DiMarcantonio and R. Cirami for suggestions and documentation. Special thanks to F. Pasian for hints and encouragement.
