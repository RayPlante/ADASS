
\bibliographystyle{asp2010}
\resetcounters

\markboth{Kamennoff, Foucaud, and Reybier}{Astrophysical Specific Language for Big Data Computation}

	\title{Development of an Astrophysical Specific Language for Big Data Computation}
	\author{Nicolas~Kamennoff,$^1$ Sebastien~Foucaud,$^2$ and Sebastien~Reybier$^3$}
	\affil{$^1$ACSEL / Epitech, 14 rue Voltaire 94270 Le Kremlin-Bicêtre, France}
	\affil{$^2$National Taiwan Normal University, Department of Earth Sciences, 88~Tingzhou~Road, Sec. 4, Wenshan district, Taipei 11677, Taiwan}
	\affil{$^3$Software and Mind Innovation, 5 allée de la musique, 95210 Saint-Gratien, France}

\aindex{Kamennoff, N.}
\aindex{Foucaud, S.}
\aindex{Reybier, S.}

	\begin{abstract}
		This paper presents general issues emphasized by the new era of Extreme Intensive Data Computation. We have analyzed and spotted the first three major problems: resource optimization, heterogeneous software ecosystems, and data transfer. From this point we advocate for a middleware solution offering a very modular and maintainable system for data analysis. As computations must be designed and described by specialists in astronomy, we aim to define a friendly specific programming language. Thus researchers will be able to configure and express their requests in a manner abstracted from computer science specific parts. With this method we expect substantial benefits in computing capabilities in data analysis.
	\end{abstract}

	\section{Context}
		Astrophysics faces data issues. Resources available to gather data overpass resources dedicated to their analysis. Upcoming observatories such as the \ssindex{observatories!Earth-based!LSST}LSST and \ssindex{observatories!Earth-based!SKA}SKA will not help to lessen this problem. But it is a challenge that is addressed in the Information Technology field as, considering the amount of public data, an issue encountered by astrophysics that is one step beyond them. Along setting up a new VO in Taiwan (\ssindex{data centers!TWEA-DC}TWEA-DC), we aim at designing an open-source, distributed solution to enhance data analysis capabilities.

	\section{Selected Issues}
		\label{BLINK_ISSUES}
		The data analysis issue has multiple facets, thus we start by presenting the ones we consider having the most benefit. These problems can be divided into 3 freehand classes: resource usage, heterogeneous software ecosystems, and data transfer.
		
		\subsection{Resource Usage}
			A vast majority of software were (and still are) developed considering that hardware capabilities will increase indefinitely. Thus, for twenty years, very few software developers really focus on optimization. Indeed, many focus on tools, methods, and languages that enable users to quickly develop new software and tools. Alas, as we are facing a deluge of data, computation resources do not follow the growth of our needs.
			
			Plus, hardware is evolving due to electro-migration and sub-threshold conduction phenomenons. For a decade now these problems have kept manufacturers from increasing the speed of modern processors. So they turn to multiplying processing units, which leads us into the current era of parallel computing, with multi-core and multi-processor CPUs, \ssindex{computing!GPU}GPUs, APUs, etc. This generalized hardware modification requires another way of thinking about software and introduces new tricky traps. In fact, most of currently used software, in every field, does not use all resources available from the computer, or worst, uses them badly.
			
			Creating efficient software, even using a high quality framework, is a non trivial activity that requires skills, training, and a lot of time. As many people (from different sciences, including software developers themselves) create parts of their software, most do not have the skills or knowledge of the low level layers (CPU cache, unfitted containers, and data structures) and do not exploit all computing resources.

		\subsection{Heterogeneous Software Ecosystems}
			Much data analysis software is available for different mainstream tasks, many of which are standalone services. As many tools are used, a computation often requires more than one of them, letting researchers create scripts, when they do not need to create complete parts of the analysis system.
			
			The more different the software you use is, with unsuited communication system possibilities (using files or wordy protocols), the more it drastically reduces performance. Files are the worst case, as hard drives lower performance in an analysis tools chain, and performance hunters need to consider them to be unfortunate bottle-necks whenever it is applicable.

		\subsection{Data transfer}
			Virtual Observatories are used to store public data. In this configuration, users you want to access them need to create their requests using the DAL and gather them on local storage. So for each computation, data needs to be downloaded, resulting in an overhead during the computation, which can severely increase analysis time consumption. Depending on the local Internet download speed and the\ssindex{Virtual Observatory(VO)} VO upload capabilities and usage, manipulating vast amount of data represents a major issue, especially for small research laboratories.
			
			Thus, waiting for data may quickly become one of the major time consuming tasks in a computation, which is regrettable.

	\section{State of the Art}
		Some projects have already tried to confront this issue. We note the \ssindex{protocols!SAMP}SAMP protocol from the \ssindex{International Virtual Observatory Alliance (IVOA)}IVOA and the FASE project.
		
		\subsection{SAMP}
			\ssindex{protocols!SAMP}SAMP, which stands for Simple Application Messaging Protocol, is reflected by the Application Group of Interest from the \ssindex{International Virtual Observatory Alliance (IVOA)}IVOA consortium. It defines a messaging protocol recommended for handling by astronomical software in order to reduce overheads due to the interoperability between the different analysis tools and file transfer.
			
			You can access the detailed presentation from \cite{Taylor}. The \ssindex{International Virtual Observatory Alliance (IVOA)}IVOA currently recognizes that an attempt to build a monolithic tool is not a rational solution. \ssindex{protocols!SAMP}SAMP then defines how applications should be able to collaborate and share their data. This method is directly related to a part of the data transfer and heterogeneous environment issues discussed above. However, this system does not handle data localization nor interfere with optimization of resource usage from each software package. Indeed, as \ssindex{International Virtual Observatory Alliance (IVOA)}IVOA advocates for an interaction of various software packages, development specific issues are not addressed. 
			
		\subsection{FASE}
			\ssindex{packages!FASE}FASE \citep{Granet}, stands for Future Astronomical Software Environment. It defines packaging requirements enabling analysis software to run onto the OPTICON (Optical-Infrared Co-ordination Network for Astronomy) funded by the European FP7 program (\cite{Fase}). This project aims to enable software to be executed on remote computational resources.
			
			The presentation of this in \citet{Grosbol} emphasizes the accessibility to legacy software as well as its capability in enabling user scripting routines. This project relies on the \ssindex{protocols!SAMP}SAMP protocol. To the author's knowledge, \ssindex{packages!FASE}FASE is still actually a prototype designed to show the feasibility of such a solution.

	\section{Toward a Domain Specific Language}
		Working toward an unified ecosystem for astrophysical data analysis is mandatory. The \ssindex{protocols!SAMP}SAMP and \ssindex{packages!FASE}FASE projects presented above only address part of the main issues emphasized in \S\ref{BLINK_ISSUES}. If we acknowledge that a monolithic software solution is non-sense, we think that a modular distributed middleware is a viable solution. Considering the lack of resources available to develop sustainable features and software in the field, it is crucial to have a way to share and to maintain the most common parts. Indeed, lots of computations needs the same parts of algorithms, and data structures.
		
		But we also advocate that astronomers should not have to worry about IT technical stuff and programming low level layers. That why we advocate for a Domain Specific Language for astronomical data analysis.

		\subsection{Using Every Resource Available}
			As hardware improvements are reaching a physical and financial problem, it is important to use the best possible resources we have. It would be good if there were tools and framework able to easily create programs that run on multi-core CPUs, \ssindex{computing!GPU}GPUs and APUs. But as we already stated, doing it in an efficient way can be quite tricky. A framework like what we are working on should be able to select between variations of a single algorithm specialized on different hardware.

		\subsection{Strong Scheduling to Create Efficient Computation Pipelines}
			Available resources can be distant or already in use by another computation. One of the major parts of the project is to define a strong \ssindex{software!scheduling}scheduling system. Such a system will be able to gather resources dynamically, to select them for a specific request based on various characteristics: distance from the data, computation and memory capabilities, availability, reliability, etc.
			
			As we intend to offer an easy to install system, we have to design a strong \ssindex{software!scheduling}scheduling service able to learn and monitor its resources: using graph learning, graph mining, and decision machine learning. Such a system would also be able to run simulation test. Then in a second part, we look for data \ssindex{data!mining}mining and input categorization that will enable this \ssindex{software!scheduling}scheduling service to efficiently choose how to compute requests considering the request and the data to compute.

		\subsection{DSPL for Strong Modularity and Good Performance}
			Dynamic Software Product Line (DSPL) is a new interest in Computer Science \citep{DSPL}. This is a rising paradigm that focuses on software adaptation capabilities to runtime instead of forging it during compilation. This could allow one to change running software to fit the new computation needs and still reaching near optimal performance.
			
			This way, we can create and adapt computation \ssindex{data!pipelines!reduction}pipelines on-the-fly which will fulfill users' requests as fast as possible.
		
		\subsection{In Memory Processing}
			Input and output latency is another disabling bottleneck. For example, getting, storing, and updating information on a hard drive. Getting uniform and strongly scheduled pipelines to complete requested computations will allow us to ensure a maximum of in-memory computation. Random Access Memory (RAM) indeed has performance which is far more efficient for this. Thus we can abstract how to apply and fine tune this methodology from scientists in other disciplines.
			
		\subsection{Next Steps and Expected Roadmap}
			We are currently working on the first tools, an \ssindex{methods!indexing!Hierarchical Triangular Mesh (HTM)}HTM Quadtree algorithm of CPU, \ssindex{computing!GPU}GPU and APU architectures as part of a \ssindex{cross-matching!services}cross-matching service we hope to provide through the \ssindex{data centers!TWEA-DC}TWEA-DC during Spring 2013. Concurrently we are designing the standalone services of computation resources and the basic features of the \ssindex{software!scheduling}scheduling service. We are also leading a Space Partitioning Survey on \ssindex{computing!GPU}GPU with American Micro Device (AMD) which should be released next summer.
			
			As this project aims to design a new simplified programming language for the astronomical research area, it requires a close collaboration between astronomers and IT specialists. We need to start discussing as soon as possible about the DSL specification, format and important first features. A dedicated website and its community tools should be released by the end of the year. We invite you to check back as we will publish information on the standard mailing lists soon.

\bibliography{editor}
