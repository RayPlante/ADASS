\resetcounters
\markboth{Lee and Budav\'ari}{Cross-Identification of Astronomical Catalogs on Multiple \ssindex{computing!GPU}GPUs}


\title{Cross-Identification of Astronomical Catalogs on Multiple \ssindex{computing!GPU}GPUs}
\author{Matthias~A.~Lee and Tam\'as~Budav\'ari
\affil{Johns Hopkins University, 3701 San Martin Drive, Baltimore, MD 21218, USA}
}

\aindex{Lee, M. A.}
\aindex{Budav{\'a}ri, T.}

\begin{abstract}
One of the most fundamental problems in observational astronomy is the cross-identification of sources. Observations are made in different wavelengths, at different times, and from different locations and instruments, resulting in a large set of independent observations. The scientific outcome is often limited by our ability to quickly perform meaningful associations between detections. The matching, however, is difficult scientifically, statistically, as well as computationally. The former two require detailed physical modeling and advanced probabilistic concepts; the latter is due to the large volumes of data and the problem's combinatorial nature. In order to tackle the computational challenge and to prepare for future surveys, whose measurements will be exponentially increasing in size past the scale of feasible CPU-based solutions, we developed a new implementation which addresses the issue by performing the associations on multiple Graphics Processing Units (\ssindex{computing!GPU}GPUs). Our implementation utilizes up to 6 \ssindex{computing!GPU}GPUs in combination with the \ssindex{libraries!Thrust}Thrust library to achieve an over 40x speed up verses the previous best implementation running on a multi-CPU SQL Server.
\end{abstract}


\section{Introduction}
With the advance of technology, modern sensors have become cheaper, smaller and more effective. Proportionately the amount of data collected grows exponentially. This growth is especially true in the field of astronomy where modern telescopes can collect millions/billions of data-points in a single day. Observations are made in different wavelengths, at different times, and from different locations and instruments, resulting in a large set of independent observations with variations based on \textit{what} was observed by \textit{which} instrument at \textit{what} point in time.

Cross-referencing or cross-identifying common objects between multiple surveys can lead to new discoveries and breakthroughs as each survey contributes its own unique datapoints. The problem is correctly cross-identifying the same objects across all different observations, taking into account all factors. Due to the growth in size of these datasets it has become increasingly computationally difficult and time consuming to cross-identify these objects. State-of-the-art systems such as SkyServer.sdss.org cross-match 50m objects(\ssindex{catalogs!individual!GALEX}GALEX) to 150m(\ssindex{surveys!Sloan Digital Sky Survey(SDSS)}SDSS) in 1 hour when parallelized across multiple CPUs. This problem is extremely computationally intensive, if we consider 2 catalogs, A and B, of size n and m, respectively, we need to make n*m comparisons. Which in this small case would be 7.5 Billion comparisons. We describe a method of doing a cross-match on 2 catalogs 3x larger in under 4 minutes. That yields an over 40x speedup!

\section{Divide and Conquer}
The first challenge in implementing \ssindex{cross-matching}cross-matching is to adequately chunk the sky into pieces so that it parallelizes well on \ssindex{computing!GPU}GPUs. As the end goal is to parallelize across not just one, but multiple \ssindex{computing!GPU}GPUs, the chunking has to be effective at both the global and local scale of our processing \ssindex{data!pipelines!reduction}pipeline. The na\"{i}ve approach to this problem would be comparing every datapoint of catalog A to every datapoint in catalog B. Lets remember that we are looking to identify common object across the catalogs. It is very unlikely that a common object is in completely different regions of the 2 catalogs. Gray et al. suggest in "There Goes the Neighborhood: Relational Algebra for Spatial Data Search" \citet{gray2004there} and "The \ssindex{algorithm!zones}Zones Algorithm for Finding Points-Near-a-Point or \ssindex{cross-matching}Cross-Matching Spatial Datasets" \citet{gray2007zones} to map the catalogs into horizontal zones of a height zoneHeight, where zoneHeight is measured in archseconds. (see Figure~\ref{zones})
\articlefigure[scale=.4]{part8/Lee_P40/P040_f1.eps}{zones}{A Catalog divided up into Segments and Zones. Also visible is our object O and its search radius $\theta$}

If we aim to find all matches within a radius $\theta$ of object O, we find the min. and max. zone enclosing our search radius in the range of declination:
\begin{eqnarray}
minZone = [(dec - \theta) / zoneHeight]\nonumber\\
maxZone = [(dec + \theta) / zoneHeight]
\end{eqnarray}
In Figure 1 minZone and maxZone would be Zone\_1 and Zone\_3. We also compute the search radius bounds in the range of RA(right ascension). Given the min and max Zones as well as the RA range, we have successfully narrowed our search field

\section{Parallel on Multiple \ssindex{computing!GPU}GPUs}
From here on we move to the parallel implementation. We start by dividing the catalog into multiple segments so that two segments can fit on 1 \ssindex{computing!GPU}GPU at a time. We then utilize the \ssindex{libraries!Thrust}Thrust library to sort each segment by declination (see Figure \ref{pipeline}) to allow us to break each segment into zones. Next we create Jobs. Each Job is a unique match between one segment of catalog A and one segment of catalog B, such that all combinations are covered. Ex: SegA\_0 x SegB\_0, SegA\_0 x SegB\_1 and so on. These Jobs are passed off to the Job manager which stores them in a Job Queue. The Job Manager is the entity which provides jobs to each processing thread. When a processing thread finishes a Job it requests a new one. To optimize memory transfers, between the host and the device, the Job Manager prefers to give out Jobs based on which segments are left in memory from the previous Job, therefore cutting down duplicate transfers.
\articlefigure[scale=.5]{part8/Lee_P40/P040_f2.eps}{pipeline}{Reading, Splitting and Sorting of Catalogs into segments.}

\section{Kernel Optimization}
For each zone in Segment A, the worker thread launches as many kernels as there are zones within range $\theta$ in Segment B. Every thread of each of these kernel's calculates the vector distance between a unique pair of objects, one object from each segment. If the computed distance is below a certain threshold, the result is marked "found". Many of the calculated distances will be greater than the threshold, resulting in a sparsely populated results array. We utilize this fact to minimize the data we need to return. In our current implementation we compact the "found" results after every kernel run into a global array using a parallel prefix scan as described in "Parallel Prefix Sum (Scan) with \ssindex{computing!architecture!CUDA}CUDA" \citet{harris2007parallel}.

\section{Results}
Our results show a greater than 40x speedup over the previous best implementation based on multi CPU performance on SQL Server (see Figure 3). Our speedups are slightly sub-linear, but that can be expected as the overhead of controlling worker threads grows, speedups decrease. The larger the segment size we can keep in memory the better we perform on a same size problem. This is due to the overhead of launching sorters and passing jobs back and forth. Theoretically Nvida c2050s support 2 segments of 50 million objects, but we ran into glitches with our kernels and the overhead required by the \ssindex{libraries!Thrust}Thrust library for sorting and copying segments. This is a work-in-progress. In the future we would like to attempt adding dynamic segment size scaling based on the memory of available \ssindex{computing!GPU}GPUs. We have released a public version of our code on \ssindex{code!repository!GitHub}GitHub. Please visit \url{http://matthiaslee.github.com/Xmatch} or email us if you are interested in using and/or contributing to this project.
\articlefigure[scale=0.4]{part8/Lee_P40/P040_f3.eps}{Lresults}{Speedup and Scaleup across \ssindex{computing!GPU}GPU count and segment size} 

\bibliographystyle{asp2010}
\bibliography{editor}
